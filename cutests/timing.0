GPTL was built without threading
HAVE_LIBMPI was false
HAVE_PAPI was false
ENABLE_NESTEDOMP was false
Autoprofiling capability was NOT enabled
Underlying timing routine was gettimeofday.
GPTLget_overhead: using hash entry 317=total for getentry estimate
Total overhead of 1 GPTL start or GPTLstop call=8.4e-08 seconds
Components are as follows:
Fortran layer:             0.0e+00 =   0.0% of total
Get thread number:         2.0e-09 =   2.4% of total
Generate hash index:       2.4e-08 =  28.6% of total
Find hashtable entry:      5.0e-09 =   6.0% of total
Underlying timing routine: 5.1e-08 =  60.7% of total
Misc start/stop functions: 2.0e-09 =   2.4% of total

NOTE: If GPTL is called from C not Fortran, the 'Fortran layer' overhead is zero
NOTE: For calls to GPTLstart_handle()/GPTLstop_handle(), the 'Generate hash index' overhead is zero
NOTE: For auto-instrumented calls, the cost of generating the hash index plus finding
      the hashtable entry is 1.0e-09 not the 2.9e-08 portion taken by GPTLstart
NOTE: Each hash collision roughly doubles the 'Find hashtable entry' cost of that timer

If overhead stats are printed, they are the columns labeled self_OH and parent_OH
self_OH is estimated as 2X the Fortran layer cost (start+stop) plust the cost of 
a single call to the underlying timing routine.
parent_OH is the overhead for the named timer which is subsumed into its parent.
It is estimated as the cost of a single GPTLstart()/GPTLstop() pair.
Print method was full_tree.

If a '%_of' field is present, it is w.r.t. the first timer for thread 0.
If a 'e6_per_sec' field is present, it is in millions of PAPI counts per sec.

A '*' in column 1 below means the timer had multiple parents, though the values
printed are for all calls. Multiple parent stats appear later in the file in the
section titled 'Multiple parent info'
A '!' in column 1 means the timer is currently ON and the printed timings are only
valid as of the previous GPTLstop. '!' overrides '*' if the region had multiple
parents and was currently ON.

Process size=13075 MB rss=100 MB

Stats for thread 0:
            Called  Recurse     Wall      max      min   selfOH parentOH
  total          1     -       0.949    0.949    0.949    0.000    0.000

Overhead sum =  1.66e-07 wallclock seconds
Total calls  = 1
Size of hash table was 1023
Mean hash index for thread 0 was 317.000000

Total GPTL memory usage = 17.616 KB
Components:
Hashmem                 = 16.376 KB
Regionmem               = 0.208 KB (papimem portion = 0 KB)
Parent/child arrays     = 0.008 KB
Callstackmem            = 1.024 KB

GPTLthreadid[0] = 0


GPU Results:
GPTLprint_gpustats: device number=0
GPTLprint_gpustats: hostname=laptop2018
Underlying timing routine was clock64() assumed @ 1.493000 Ghz
Total overhead of 1 GPTLstart_gpu + GPTLstop_gpu pair call=3.7e-06 seconds
Components of the pair are as follows (Fortran layer ignored):
NOTE: sum of percentages should be near 100 percent but not necessarily exact
This is because start/stop timing est. is done separately from components
Get warp number:                6.5e-07 =  17.9% of total
Underlying timing routine+SMID: 6.8e-08 =   1.9% of total
Misc calcs in GPTL_start_gpu:   8.4e-07 =  23.1% of total
Misc calcs in GPTL_stop_gpu:    1.4e-06 =  38.2% of total

my_strlen:                      7.7e-07 (name=GPTL_ROOT)
STRMATCH:                       9.3e-07 (matched name=GPTL_ROOT)


GPU timing stats
GPTL could handle up to 20 warps (640 threads)
This setting can be changed with: GPTLsetoption(GPTLmaxthreads_gpu,<number>)
19 = max warpId found
19 = max warpId examined
Only warps which were timed are counted in the following stats
Overhead estimates self_OH and parent_OH are for warp with 'maxcount' calls
OHD estimate assumes Fortran, and non-handle routines used
Actual overhead can be reduced by using 'handle' routines and '_c' Fortran routines

name            = region name
calls           = number of invocations across all examined warps
warps           = number of examined warps for which region was timed at least once
holes           = number of examined warps for which region was never executed (maxwarpid_timed + 1 - nwarps(region)
wallmax (warp)  = max wall time (sec) taken by any timed warp for this region, followed by the warp number
wallmin (warp)  = min wall time (sec) taken by any timed warp for this region, followed by the warp number
maxcount (warp) = max number of times region invoked by any timed warp, followed by the warp number
mincount (warp) = min number of times region invoked by any timed warp, followed by the warp number
negmax (warp)   = if a region had a negative interval, biggest count is printed along with the warp number responsible
nwarps          = number of warps encountering a negative interval
Bad_SM          = number of times smid changed (these instances are NOT timed!) Max possible = 'calls'
self_OH         = estimate of GPTL overhead (sec) in the timer incurred by 'maxcount' invocations of it
parent_OH       = estimate of GPTL overhead (sec) in the parent of the timer incurred by 'maxcount' invocations of it

   name    calls  warps  holes | wallmax  (warp)| wallmin (warp) | maxcount (warp)| mincount (warp)| negmax (warp) nwarps  | Bad_SM| self_OH parent_OH
  runit       20     20      0 |   1.000     18 |   1.000      5 |       1      0 |       1      0 |    -       -      -   |   -   |8.78e-07  2.08e-06 
percall       20     20      0 |   1.000      1 |   1.000     13 |       1      0 |       1      0 |    -       -      -   |   -   |8.78e-07  2.08e-06 

GPTL GPU memory usage (Timers)      =     38.4 KB
GPTL GPU memory usage (Timer names) =     1.92 KB
                                      --------
GPTL GPU memory usage (Total)       =    40.32 KB
